import os
import json
import time
import requests
import pandas as pd
from tqdm import tqdm
from datetime import datetime, timedelta

# --- CONFIGURA√á√ÉO ---
# Se voc√™ setou a vari√°vel de ambiente: $env:GITHUB_TOKEN="seu_token"
# Caso contr√°rio, substitua o valor do TOKEN abaixo.
TOKEN = os.environ.get("GITHUB_TOKEN", "SEU_GITHUB_TOKEN_AQUI") 

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")

os.makedirs(DATA_DIR, exist_ok=True)

OUTPUT_CSV = os.path.join(DATA_DIR, "cloned_repos.csv")
TARGET_REPOS = 200
MIN_PRS_WITH_REVIEW = 100
MIN_PR_LIFESPAN_HOURS = 1 # PR deve ter levado pelo menos 1 hora para ser fechado/mesclado

# --- FUN√á√ïES AUXILIARES ---

def handle_rate_limit(response, fallback_wait=600):
    """Lida com erros de limite de taxa (Rate Limit) da API do GitHub."""
    if response.status_code == 403 and int(response.headers.get("X-RateLimit-Remaining", 1)) == 0:
        reset_timestamp = int(response.headers.get("X-RateLimit-Reset", time.time() + fallback_wait))
        now = int(time.time())
        wait_seconds = max(reset_timestamp - now + 10, 10) # Espera at√© o reset + 10s de buffer
        
        tqdm.write(f"\nüö¶ Limite de requisi√ß√µes da API atingido (403).")
        tqdm.write(f"‚è≥ Esperando {wait_seconds} segundos at√© a libera√ß√£o.")
        time.sleep(wait_seconds)
        return True
    
    if response.status_code != 200:
        tqdm.write(f"\n‚ö†Ô∏è Erro na requisi√ß√£o: {response.status_code} - {response.reason}")
        # Tenta esperar um pouco por outros erros tempor√°rios
        time.sleep(10) 
        return True
        
    return False

def filter_repos_with_min_prs(token, needed=TARGET_REPOS, min_prs=MIN_PRS_WITH_REVIEW):
    """
    Usa a API REST para buscar repos e a API GraphQL para verificar PRs revisados.
    Inclui a filtragem de PRs com tempo de vida m√≠nimo.
    """
    headers_rest = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github.v3+json"
    }
    headers_graphql = {
        "Authorization": f"bearer {token}"
    }

    all_filtered = []
    page = 1
    max_pr_fetch = 100 # N√∫mero de PRs por requisi√ß√£o GraphQL
    
    print(f"üîç Iniciando coleta de reposit√≥rios: +1000 estrelas e pelo menos {min_prs} PRs v√°lidos...")

    while len(all_filtered) < needed:
        # 1. API REST: Busca reposit√≥rios por popularidade
        params = {
            "q": "stars:>1000",
            "sort": "stars",
            "order": "desc",
            "per_page": 100,
            "page": page
        }

        response = requests.get("https://api.github.com/search/repositories", headers=headers_rest, params=params)
        
        if handle_rate_limit(response):
            continue

        repos = response.json().get("items", [])
        if not repos:
            print("\nüèÅ N√£o h√° mais reposit√≥rios para buscar na API REST.")
            break

        tqdm.write(f"\nüîÑ Processando p√°gina {page} da API REST ({len(repos)} reposit√≥rios)...")
        
        # 2. API GraphQL: Filtra PRs por review e tempo de vida
        repos_nesta_pagina = []
        
        for repo in tqdm(repos, desc=f"  ‚öôÔ∏è Filtrando PRs revisados e com tempo > {MIN_PR_LIFESPAN_HOURS}h", ncols=120):
            owner, name = repo["full_name"].split("/")
            valid_prs_count = 0
            cursor = None
            
            # Limita a busca para evitar requisi√ß√µes infinitas e muito longas
            max_prs_to_check = 1000 # Verifica at√© 1000 PRs mais recentes do reposit√≥rio
            checked_count = 0
            
            while checked_count < max_prs_to_check:
                after_clause = f', after: "{cursor}"' if cursor else ""
                
                # Consulta para buscar PRs fechados/mesclados, reviews e datas
                query = {
                    "query": f"""
                    {{
                        repository(owner: \"{owner}\", name: \"{name}\") {{
                            pullRequests(states: [MERGED, CLOSED], first: {max_pr_fetch}{after_clause}, orderBy: {{field: CREATED_AT, direction: DESC}}) {{
                                nodes {{
                                    reviews {{ totalCount }}
                                    createdAt
                                    closedAt
                                }}
                                pageInfo {{ hasNextPage endCursor }}
                            }}
                        }}
                    }}
                    """
                }

                try:
                    r = requests.post("https://api.github.com/graphql", headers=headers_graphql, json=query)
                    
                    if handle_rate_limit(r):
                        # Se o rate limit for atingido, a pausa j√° ocorreu, tenta o loop novamente.
                        continue 
                        
                    response_json = r.json()

                    # Lida com erros do GraphQL, como reposit√≥rio n√£o encontrado
                    if "errors" in response_json or not response_json.get("data", {}).get("repository", {}).get("pullRequests"):
                         break

                    pr_data = response_json["data"]["repository"]["pullRequests"]
                    
                    for pr in pr_data["nodes"]:
                        # 1. Checa se o PR tem review
                        has_review = pr["reviews"]["totalCount"] > 0

                        # 2. Checa o crit√©rio de tempo de vida (Closed - Created > 1 hora)
                        pr_closed_at = pr.get("closedAt")
                        pr_created_at = pr.get("createdAt")
                        
                        time_valid = False
                        if pr_closed_at and pr_created_at:
                            try:
                                t_closed = datetime.fromisoformat(pr_closed_at.replace('Z', '+00:00'))
                                t_created = datetime.fromisoformat(pr_created_at.replace('Z', '+00:00'))
                                
                                # Verifica se a diferen√ßa de tempo √© maior que 1 hora
                                time_diff = t_closed - t_created
                                if time_diff > timedelta(hours=MIN_PR_LIFESPAN_HOURS):
                                    time_valid = True
                            except:
                                # Ignora se as datas estiverem inv√°lidas
                                pass

                        if has_review and time_valid:
                            valid_prs_count += 1

                    checked_count += len(pr_data["nodes"])
                    
                    if not pr_data["pageInfo"]["hasNextPage"] or valid_prs_count >= min_prs:
                        break # Para se n√£o houver mais p√°ginas ou se j√° atingiu o m√≠nimo
                    
                    cursor = pr_data["pageInfo"]["endCursor"]

                except Exception as e:
                    tqdm.write(f"  ‚ùå Erro ao buscar PRs do repo {repo['full_name']}: {type(e).__name__} - {e}")
                    break
            
            # 3. Verifica se o reposit√≥rio √© v√°lido
            if valid_prs_count >= min_prs:
                repo["pr_count"] = valid_prs_count
                repos_nesta_pagina.append(repo)
        
        all_filtered.extend(repos_nesta_pagina)
        
        print(f"\n‚úÖ P√°gina {page} finalizada. Total acumulado: {len(all_filtered)} reposit√≥rios v√°lidos.")
        print("-" * 50)
        
        # 4. Salva o progresso
        if all_filtered:
            save_repos_to_files(all_filtered, OUTPUT_CSV)

        if len(all_filtered) >= needed:
            break
            
        page += 1

    return all_filtered[:needed]

def save_repos_to_files(repos, file_path):
    """Salva a lista de reposit√≥rios em um arquivo CSV."""
    if not repos:
        print("üî¥ Nenhum reposit√≥rio v√°lido coletado. Arquivos n√£o foram salvos.")
        return

    # Colunas requeridas no output
    selected = [
        "id", "full_name", "description", "language",
        "stargazers_count", "forks_count", "open_issues_count", "pr_count"
    ]
    
    rows = [{k: r.get(k) for k in selected} for r in repos]
    df = pd.DataFrame(rows)

    # Limita o tamanho da descri√ß√£o para evitar problemas de visualiza√ß√£o
    df["description"] = df["description"].apply(
        lambda x: (x[:300] + "...") if isinstance(x, str) and len(x) > 300 else x
    )

    try:
        # Salva CSV com separador padr√£o (v√≠rgula)
        df.to_csv(file_path, index=False, encoding="utf-8")
        tqdm.write(f"  üíæ {len(repos)} reposit√≥rios salvos em {file_path}")
    except Exception as e:
        tqdm.write(f"üî¥ Erro ao salvar o arquivo CSV: {e}")

# --- EXECU√á√ÉO PRINCIPAL ---
def main():
    if TOKEN == "SEU_GITHUB_TOKEN_AQUI":
        print("üî¥ ERRO: Por favor, substitua 'SEU_GITHUB_TOKEN_AQUI' ou configure a vari√°vel de ambiente GITHUB_TOKEN.")
        return
        
    start_time = time.time()
    
    # Inicia a coleta
    filtered_repos = filter_repos_with_min_prs(TOKEN)

    # 5. Filtragem e salvamento final
    final_repos = filtered_repos[:TARGET_REPOS]
    save_repos_to_files(final_repos, OUTPUT_CSV)
    
    print("\n" + "=" * 50)
    print(f"üéâ Processo Finalizado.")
    print(f"Total de reposit√≥rios v√°lidos salvos: {len(final_repos)}")
    print(f"Arquivo de sa√≠da: {OUTPUT_CSV}")
    print(f"Tempo total de execu√ß√£o: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}")
    print("=" * 50)

if __name__ == "__main__":
    main()